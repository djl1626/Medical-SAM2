{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Size Changes:\n",
    "\n",
    "1. Update args dictionary in notebook\n",
    "2. Update GlaucomaDataset to resize to appropriate size\n",
    "3. Update feat_sizes\n",
    "4. Update the sam_prompt_encoder (via notebook cell 3)\n",
    "5. Update _bb_feat_sizes in sam2_image_predictor.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../unet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mGlaucomaDataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GlaucomaDatasetBoundingBoxes\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Training imports\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/medsam2/lib/python3.12/site-packages/sklearn/__init__.py:84\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     81\u001b[0m         __check_build,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     82\u001b[0m         _distributor_init,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     83\u001b[0m     )\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_show_versions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show_versions\n\u001b[1;32m     87\u001b[0m     __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     88\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalibration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshow_versions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    131\u001b[0m     ]\n",
      "File \u001b[0;32m~/.conda/envs/medsam2/lib/python3.12/site-packages/sklearn/base.py:19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HTMLDocumentationLinkMixin, estimator_html_repr\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_metadata_requests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester, _routing_enabled\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints\n",
      "File \u001b[0;32m~/.conda/envs/medsam2/lib/python3.12/site-packages/sklearn/utils/__init__.py:11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _joblib, metadata_routing\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bunch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Bunch\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_chunking\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gen_batches, gen_even_slices\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m estimator_html_repr\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Make _safe_indexing importable from here for backward compat as this particular\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# helper is considered semi-private and typically very useful for third-party\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# libraries that want to comply with scikit-learn's estimator API. In particular,\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# _safe_indexing was included in our public API documentation despite the leading\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# `_` in its name.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/medsam2/lib/python3.12/site-packages/sklearn/utils/_chunking.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interval, validate_params\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchunk_generator\u001b[39m(gen, chunksize):\n\u001b[1;32m     12\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Chunk generator, ``gen`` into lists of length ``chunksize``. The last\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m    chunk may have a length less than ``chunksize``.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/medsam2/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumbers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Integral, Real\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m csr_matrix, issparse\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _is_arraylike_not_scalar\n",
      "File \u001b[0;32m~/.conda/envs/medsam2/lib/python3.12/site-packages/scipy/__init__.py:51\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m __numpy_version__\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__config__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show \u001b[38;5;28;01mas\u001b[39;00m show_config\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     53\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mError importing SciPy: you cannot import SciPy while\u001b[39m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124m    being in scipy source directory; please exit the SciPy source\u001b[39m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124m    tree first and relaunch your Python interpreter.\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:991\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1087\u001b[0m, in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1187\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# General packages\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Medical-SAM2 imports\n",
    "from sam2_train.sam2_image_predictor import SAM2ImagePredictor\n",
    "from sam2_train.build_sam import build_sam2\n",
    "from sam2_train.modeling.sam2_utils import MLP\n",
    "\n",
    "# Data processing\n",
    "sys.path.append(\"../unet\")\n",
    "from GlaucomaDataset import GlaucomaDatasetBoundingBoxes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Training imports\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the base SAM2 small model\n",
    "med_sam_2 = build_sam2(\"sam2_hiera_s\", \"./checkpoints/sam2_hiera_small.pt\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_sam_2.sam_mask_decoder.num_multimask_outputs = 2\n",
    "med_sam_2.sam_mask_decoder.num_mask_tokens = 1 + med_sam_2.sam_mask_decoder.num_multimask_outputs\n",
    "med_sam_2.sam_mask_decoder.mask_tokens = nn.Embedding(med_sam_2.sam_mask_decoder.num_mask_tokens, med_sam_2.sam_mask_decoder.transformer_dim)\n",
    "med_sam_2.sam_mask_decoder.output_hypernetworks_mlps = nn.ModuleList(\n",
    "            [\n",
    "                MLP(med_sam_2.sam_mask_decoder.transformer_dim, med_sam_2.sam_mask_decoder.transformer_dim, med_sam_2.sam_mask_decoder.transformer_dim // 8, 3)\n",
    "                for i in range(med_sam_2.sam_mask_decoder.num_mask_tokens)\n",
    "            ]\n",
    "        )\n",
    "med_sam_2.sam_mask_decoder.iou_prediction_head = MLP(\n",
    "            med_sam_2.sam_mask_decoder.transformer_dim,\n",
    "            256,\n",
    "            med_sam_2.sam_mask_decoder.num_mask_tokens,\n",
    "            3,\n",
    "            sigmoid_output=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture updates for working with different image sizes\n",
    "\n",
    "# 256x256 input images \n",
    "# med_sam_2.sam_prompt_encoder.image_embedding_size = (16, 16)\n",
    "# med_sam_2.sam_prompt_encoder.input_image_size = (256, 256)\n",
    "# med_sam_2.sam_prompt_encoder.mask_input_size = (64, 64)\n",
    "# med_sam_2.image_size = 256\n",
    "# med_sam_2.sam_mask_decoder.num_mask_tokens = 2\n",
    "\n",
    "\n",
    "# 512x512 input images\n",
    "# med_sam_2.sam_prompt_encoder.image_embedding_size = (32, 32)\n",
    "# med_sam_2.sam_prompt_encoder.input_image_size = (512, 512)\n",
    "# med_sam_2.sam_prompt_encoder.mask_input_size = (128, 128)\n",
    "# med_sam_2.image_size = 512\n",
    "# med_sam_2.sam_mask_decoder.num_mask_tokens = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_score(pred_mask: torch.Tensor, gt_mask: torch.Tensor, smooth: float = 1e-6) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Calculate the Dice score for a single pair of predicted and ground truth masks.\n",
    "\n",
    "    Args:\n",
    "        pred_mask (torch.Tensor): Predicted mask of shape [H, W] with values between 0 and 1.\n",
    "        gt_mask (torch.Tensor): Ground truth mask of shape [H, W] with binary values {0, 1}.\n",
    "        smooth (float): A small constant added to avoid division by zero.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The Dice score (scalar tensor).\n",
    "    \"\"\"\n",
    "    # Flatten the masks to 1D vectors\n",
    "    pred_flat = pred_mask.contiguous().view(-1)\n",
    "    gt_flat   = gt_mask.contiguous().view(-1)\n",
    "    \n",
    "    # Compute the intersection and the sums of masks\n",
    "    intersection = (pred_flat * gt_flat).sum()\n",
    "    sum_masks = pred_flat.sum() + gt_flat.sum()\n",
    "    \n",
    "    # Compute Dice score\n",
    "    dice = (2.0 * intersection + smooth) / (sum_masks + smooth)\n",
    "    return dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for displaying masks and prompts on final predicted masks\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def show_mask(mask, ax, random_color=False, borders=True):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask = mask.astype(np.uint8)\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    if borders:\n",
    "        import cv2\n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "        contours = [cv2.approxPolyDP(contour, epsilon=0.01, closed=True) for contour in contours]\n",
    "        mask_image = cv2.drawContours(mask_image, contours, -1, (1, 1, 1, 0.5), thickness=2)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "def show_points(coords, labels, ax, marker_size=375):\n",
    "    pos_points = coords[labels == 1]\n",
    "    neg_points = coords[labels == 0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0, 0, 0, 0), lw=2))\n",
    "\n",
    "def show_masks(image, masks, scores, point_coords=None, box_coords=None, input_labels=None, borders=True):\n",
    "    num_masks = len(masks)\n",
    "    fig, axs = plt.subplots(1, num_masks, figsize=(10 * num_masks, 10))  # Horizontal layout\n",
    "    if num_masks == 1:\n",
    "        axs = [axs]  # Ensure it's iterable\n",
    "    for i, (mask, score) in enumerate(zip(masks, scores)):\n",
    "        ax = axs[i]\n",
    "        ax.imshow(image)\n",
    "        show_mask(mask, ax, borders=borders)\n",
    "        if point_coords is not None:\n",
    "            assert input_labels is not None\n",
    "            show_points(point_coords, input_labels, ax)\n",
    "        if box_coords is not None:\n",
    "            show_box(box_coords, ax)\n",
    "        if num_masks > 1:\n",
    "            ax.set_title(\"Optic Disc Mask\" if i == 0 else \"Optic Cup Mask\", fontsize=18)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatGPT suggested changes to model architecture:\n",
    "# med_sam_2.sam_mask_decoder.mask_tokens = nn.Embedding(2, 256)\n",
    "# med_sam_2.sam_mask_decoder.output_hypernetworks_mlps = nn.ModuleList([MLP()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function for Medical-SAM2\n",
    "\n",
    "GPUdevice = torch.device('cuda')\n",
    "pos_weight = torch.ones([1]).cuda(device=GPUdevice)*2\n",
    "criterion_G = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "mask_type = torch.float32\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "def train_sam(args, net: nn.Module, optimizer, train_loader, epoch):\n",
    "    \n",
    "    # use bfloat16 for the entire notebook\n",
    "    torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "\n",
    "    if torch.cuda.get_device_properties(0).major >= 8:\n",
    "        # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "    \n",
    "    # train mode\n",
    "    net.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # init\n",
    "    epoch_loss = 0\n",
    "    epoch_dice = 0\n",
    "    memory_bank_list = []\n",
    "    lossfunc = criterion_G\n",
    "\n",
    "    # Update to match the input image sizes\n",
    "    feat_sizes = [(256, 256), (128, 128), (64, 64)] # 1024x1024 images\n",
    "    # feat_sizes = [(64, 64), (32, 32), (16, 16)] # 256x256 images\n",
    "    # feat_sizes = [(128,128), (64,64), (32,32)] # 512x512 images\n",
    "\n",
    "\n",
    "    with tqdm(total=len(train_loader), desc=f'Epoch {epoch}', unit='img') as pbar:\n",
    "        for ind, pack in enumerate(train_loader):\n",
    "            # print(f\"Running for batch {ind}.\")\n",
    "            \n",
    "            to_cat_memory = []\n",
    "            to_cat_memory_pos = []\n",
    "            to_cat_image_embed = []\n",
    "\n",
    "            # input image and gt masks\n",
    "            imgs = pack['image'].to(dtype = mask_type, device = GPUdevice)\n",
    "            masks = pack['mask'].to(dtype = mask_type, device = GPUdevice)\n",
    "            name = pack['image_filename']\n",
    "\n",
    "            # click prompt: unsqueeze to indicate only one click, add more click across this dimension\n",
    "            if 'bbox' in pack:\n",
    "                # Assuming bbox shape is [batch, 4]; adjust unsqueeze if more than one box is expected.\n",
    "                boxes = pack['bbox'].unsqueeze(1).to(device=GPUdevice, dtype=torch.float)\n",
    "            else:\n",
    "                boxes = None\n",
    "\n",
    "            '''Train image encoder'''                    \n",
    "            backbone_out = net.forward_image(imgs)\n",
    "            _, vision_feats, vision_pos_embeds, _ = net._prepare_backbone_features(backbone_out)\n",
    "            \n",
    "            # dimension hint for your future use (in reference to 1024x1024 images)\n",
    "            # vision_feats: list: length = 3\n",
    "            # vision_feats[0]: torch.Size([65536, batch, 32])\n",
    "            # vision_feats[1]: torch.Size([16384, batch, 64])\n",
    "            # vision_feats[2]: torch.Size([4096, batch, 256])\n",
    "            # vision_pos_embeds[0]: torch.Size([65536, batch, 256])\n",
    "            # vision_pos_embeds[1]: torch.Size([16384, batch, 256])\n",
    "            # vision_pos_embeds[2]: torch.Size([4096, batch, 256])\n",
    "            \n",
    "            \n",
    "\n",
    "            '''Train memory attention to condition on meomory bank'''         \n",
    "            B = vision_feats[-1].size(1)  # batch size \n",
    "            \n",
    "            if len(memory_bank_list) == 0:\n",
    "                vision_feats[-1] = vision_feats[-1] + torch.nn.Parameter(torch.zeros(1, B, net.hidden_dim)).to(device=\"cuda\")\n",
    "                vision_pos_embeds[-1] = vision_pos_embeds[-1] + torch.nn.Parameter(torch.zeros(1, B, net.hidden_dim)).to(device=\"cuda\")\n",
    "                \n",
    "            else:\n",
    "                for element in memory_bank_list:\n",
    "                    to_cat_memory.append((element[0]).cuda(non_blocking=True).flatten(2).permute(2, 0, 1)) # maskmem_features\n",
    "                    to_cat_memory_pos.append((element[1]).cuda(non_blocking=True).flatten(2).permute(2, 0, 1)) # maskmem_pos_enc\n",
    "                    to_cat_image_embed.append((element[3]).cuda(non_blocking=True)) # image_embed\n",
    "\n",
    "                memory_stack_ori = torch.stack(to_cat_memory, dim=0)\n",
    "                memory_pos_stack_ori = torch.stack(to_cat_memory_pos, dim=0)\n",
    "                image_embed_stack_ori = torch.stack(to_cat_image_embed, dim=0)\n",
    " \n",
    "                vision_feats_temp = vision_feats[-1].permute(1, 0, 2).reshape(B, -1, 64, 64) \n",
    "                vision_feats_temp = vision_feats_temp.reshape(B, -1)\n",
    "\n",
    "                image_embed_stack_ori = F.normalize(image_embed_stack_ori, p=2, dim=1)\n",
    "                vision_feats_temp = F.normalize(vision_feats_temp, p=2, dim=1)\n",
    "                similarity_scores = torch.mm(image_embed_stack_ori, vision_feats_temp.t()).t()\n",
    "                \n",
    "                similarity_scores = F.softmax(similarity_scores, dim=1) \n",
    "                sampled_indices = torch.multinomial(similarity_scores, num_samples=B, replacement=True).squeeze(1)  # Shape [batch_size, 16]\n",
    "\n",
    "                memory_stack_ori_new = (memory_stack_ori[sampled_indices].squeeze(3).permute(1, 2, 0, 3))\n",
    "                memory = memory_stack_ori_new.reshape(-1, memory_stack_ori_new.size(2), memory_stack_ori_new.size(3))\n",
    "\n",
    "                memory_pos_stack_new = (memory_pos_stack_ori[sampled_indices].squeeze(3).permute(1, 2, 0, 3))\n",
    "                memory_pos = memory_pos_stack_new.reshape(-1, memory_stack_ori_new.size(2), memory_stack_ori_new.size(3))\n",
    "\n",
    "\n",
    "                vision_feats[-1] = net.memory_attention(\n",
    "                    curr=[vision_feats[-1]],\n",
    "                    curr_pos=[vision_pos_embeds[-1]],\n",
    "                    memory=memory,\n",
    "                    memory_pos=memory_pos,\n",
    "                    num_obj_ptr_tokens=0\n",
    "                    )\n",
    "\n",
    "\n",
    "            feats = [feat.permute(1, 2, 0).reshape(B, -1, *feat_size) \n",
    "                     for feat, feat_size in zip(vision_feats[::-1], feat_sizes[::-1])][::-1]\n",
    "            \n",
    "            image_embed = feats[-1]\n",
    "            high_res_feats = feats[:-1]\n",
    "\n",
    "            # for a, feat in enumerate(feats):\n",
    "            #     print(f'feat[{a}]: {feat.shape}')\n",
    "            \n",
    "            # feats[0]: torch.Size([batch, 32, 256, 256]) #high_res_feats part1\n",
    "            # feats[1]: torch.Size([batch, 64, 128, 128]) #high_res_feats part2\n",
    "            # feats[2]: torch.Size([batch, 256, 64, 64]) #image_embed\n",
    "\n",
    "            '''prompt encoder'''         \n",
    "            with torch.no_grad():\n",
    "                if (ind%5) == 0:\n",
    "                    #points=(coords_torch, labels_torch) # input shape: ((batch, n, 2), (batch, n))\n",
    "                    flag = True\n",
    "                else:\n",
    "                    points=None\n",
    "                    flag = False\n",
    "\n",
    "                se, de = net.sam_prompt_encoder(\n",
    "                    points=None,   # No point prompts used\n",
    "                    boxes=boxes,   # Use the bounding boxes from the data\n",
    "                    masks=None,\n",
    "                    batch_size=B,\n",
    "                )\n",
    "            # dimension hint for your future use\n",
    "            # se: torch.Size([batch, n+1, 256])\n",
    "            # de: torch.Size([batch, 256, 64, 64])\n",
    "            print('se:', se.shape)\n",
    "            print('de:', de.shape)\n",
    "            print('image_embed:', image_embed.shape)\n",
    "\n",
    "            '''train mask decoder'''       \n",
    "            low_res_multimasks, iou_predictions, sam_output_tokens, object_score_logits = net.sam_mask_decoder(\n",
    "                    image_embeddings=image_embed,\n",
    "                    image_pe=net.sam_prompt_encoder.get_dense_pe(), \n",
    "                    sparse_prompt_embeddings=se,\n",
    "                    dense_prompt_embeddings=de, \n",
    "                    multimask_output=True, # args.multimask_output if you want multiple masks\n",
    "                    repeat_image=False,  # the image is already batched\n",
    "                    high_res_features = high_res_feats\n",
    "                )\n",
    "            # dimension hint for your future use\n",
    "            # low_res_multimasks: torch.Size([batch, multimask_output, 256, 256])\n",
    "            # iou_predictions.shape:torch.Size([batch, multimask_output])\n",
    "            # sam_output_tokens.shape:torch.Size([batch, multimask_output, 256])\n",
    "            # object_score_logits.shape:torch.Size([batch, 1])\n",
    "            \n",
    "            \n",
    "            # resize prediction\n",
    "            pred = F.interpolate(low_res_multimasks,size=(args['out_size'],args['out_size']))\n",
    "            high_res_multimasks = F.interpolate(low_res_multimasks, size=(args['image_size'], args['image_size']),\n",
    "                                                mode=\"bilinear\", align_corners=False)\n",
    "            \n",
    "            '''memory encoder'''       \n",
    "            # new caluculated memory features\n",
    "            maskmem_features, maskmem_pos_enc = net._encode_new_memory(\n",
    "                current_vision_feats=vision_feats,\n",
    "                feat_sizes=feat_sizes,\n",
    "                pred_masks_high_res=high_res_multimasks,\n",
    "                is_mask_from_pts=flag)  \n",
    "            # dimension hint for your future use\n",
    "            # maskmem_features: torch.Size([batch, 64, 64, 64])\n",
    "            # maskmem_pos_enc: [torch.Size([batch, 64, 64, 64])]\n",
    "                \n",
    "            maskmem_features = maskmem_features.to(torch.bfloat16)\n",
    "            maskmem_features = maskmem_features.to(device=GPUdevice, non_blocking=True)\n",
    "            maskmem_pos_enc = maskmem_pos_enc[0].to(torch.bfloat16)\n",
    "            maskmem_pos_enc = maskmem_pos_enc.to(device=GPUdevice, non_blocking=True)\n",
    "\n",
    "\n",
    "            # add single maskmem_features, maskmem_pos_enc, iou\n",
    "            if len(memory_bank_list) < args['memory_bank_size']:\n",
    "                for batch in range(maskmem_features.size(0)):\n",
    "                    memory_bank_list.append([(maskmem_features[batch].unsqueeze(0)).detach(),\n",
    "                                             (maskmem_pos_enc[batch].unsqueeze(0)).detach(),\n",
    "                                             iou_predictions[batch, 0],\n",
    "                                             image_embed[batch].reshape(-1).detach()])\n",
    "            \n",
    "            else:\n",
    "                for batch in range(maskmem_features.size(0)):\n",
    "                    \n",
    "                    # current simlarity matrix in existing memory bank\n",
    "                    memory_bank_maskmem_features_flatten = [element[0].reshape(-1) for element in memory_bank_list]\n",
    "                    memory_bank_maskmem_features_flatten = torch.stack(memory_bank_maskmem_features_flatten)\n",
    "\n",
    "                    # normalise\n",
    "                    memory_bank_maskmem_features_norm = F.normalize(memory_bank_maskmem_features_flatten, p=2, dim=1)\n",
    "                    current_similarity_matrix = torch.mm(memory_bank_maskmem_features_norm,\n",
    "                                                         memory_bank_maskmem_features_norm.t())\n",
    "\n",
    "                    # replace diagonal (diagnoal always simiarity = 1)\n",
    "                    current_similarity_matrix_no_diag = current_similarity_matrix.clone()\n",
    "                    diag_indices = torch.arange(current_similarity_matrix_no_diag.size(0))\n",
    "                    current_similarity_matrix_no_diag[diag_indices, diag_indices] = float('-inf')\n",
    "\n",
    "                    # first find the minimum similarity from memory feature and the maximum similarity from memory bank\n",
    "                    single_key_norm = F.normalize(maskmem_features[batch].reshape(-1), p=2, dim=0).unsqueeze(1)\n",
    "                    similarity_scores = torch.mm(memory_bank_maskmem_features_norm, single_key_norm).squeeze()\n",
    "                    min_similarity_index = torch.argmin(similarity_scores) \n",
    "                    max_similarity_index = torch.argmax(current_similarity_matrix_no_diag[min_similarity_index])\n",
    "\n",
    "                    # replace with less similar object\n",
    "                    if similarity_scores[min_similarity_index] < current_similarity_matrix_no_diag[min_similarity_index][max_similarity_index]:\n",
    "                        # soft iou, not stricly greater than current iou\n",
    "                        if iou_predictions[batch, 0] > memory_bank_list[max_similarity_index][2] - 0.1:\n",
    "                            memory_bank_list.pop(max_similarity_index) \n",
    "                            memory_bank_list.append([(maskmem_features[batch].unsqueeze(0)).detach(),\n",
    "                                                     (maskmem_pos_enc[batch].unsqueeze(0)).detach(),\n",
    "                                                     iou_predictions[batch, 0],\n",
    "                                                     image_embed[batch].reshape(-1).detach()])\n",
    "\n",
    "            # backpropagation\n",
    "            loss_disc = lossfunc(pred[:, 0, :, :], masks[:, 0, :, :])\n",
    "            loss_cup  = lossfunc(pred[:, 1, :, :], masks[:, 1, :, :])\n",
    "            dice_disc = dice_score(torch.sigmoid(pred[:, 0, :, :]), masks[:, 0, :, :])\n",
    "            dice_cup  = dice_score(torch.sigmoid(pred[:, 1, :, :]), masks[:, 1, :, :])\n",
    "\n",
    "            loss = .3 * loss_disc + .7 * loss_cup\n",
    "            dice = (dice_disc + dice_cup) / 2\n",
    "\n",
    "            pbar.set_postfix(**{'loss (batch)': loss.item()})\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_dice += dice.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Cleanup large intermediate tensors that are no longer needed.\n",
    "            del backbone_out, vision_feats, vision_pos_embeds, feats, image_embed, high_res_feats, se, de, low_res_multimasks, iou_predictions, sam_output_tokens, object_score_logits, pred\n",
    "\n",
    "            # Optionally free cached memory.\n",
    "            if ind % 10 == 0:  # For instance, every 10 iterations\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            pbar.update()\n",
    "\n",
    "    return epoch_loss/len(train_loader), epoch_dice/len(train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation function for Medical-SAM2\n",
    "def validation_sam(args, val_loader, epoch, net: nn.Module, clean_dir=True):\n",
    "\n",
    "    # use bfloat16 for the entire notebook\n",
    "    torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "\n",
    "    if torch.cuda.get_device_properties(0).major >= 8:\n",
    "        # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "\n",
    "    # eval mode\n",
    "    net.eval()\n",
    "\n",
    "    n_val = len(val_loader) \n",
    "    threshold = (0.1, 0.3, 0.5, 0.7, 0.9)\n",
    "    GPUdevice = torch.device('cuda:' + str(0))\n",
    "\n",
    "    # init\n",
    "    lossfunc = criterion_G\n",
    "    memory_bank_list = []\n",
    "    feat_sizes = [(256, 256), (128, 128), (64, 64)] # 1024x1024\n",
    "    # feat_sizes = [(64, 64), (32, 32), (16, 16)] # 256x256\n",
    "    # feat_sizes = [(128,128), (64,64), (32,32)] # 512x512\n",
    "    total_loss = 0\n",
    "    total_eiou = 0\n",
    "    total_dice = 0\n",
    "\n",
    "\n",
    "    with tqdm(total=n_val, desc='Validation round', unit='batch', leave=False) as pbar:\n",
    "        for ind, pack in enumerate(val_loader):\n",
    "            to_cat_memory = []\n",
    "            to_cat_memory_pos = []\n",
    "            to_cat_image_embed = []\n",
    "\n",
    "            # input image and gt masks\n",
    "            imgs = pack['image'].to(dtype = mask_type, device = GPUdevice)\n",
    "            masks = pack['mask'].to(dtype = mask_type, device = GPUdevice)\n",
    "            name = pack['image_filename']\n",
    "\n",
    "            # click prompt: unsqueeze to indicate only one click, add more click across this dimension\n",
    "            if 'bbox' in pack:\n",
    "                # Assuming bbox shape is [batch, 4]; adjust unsqueeze if more than one box is expected.\n",
    "                bbox_temp = pack['bbox'].to(device=GPUdevice, dtype=torch.float)\n",
    "                boxes = bbox_temp.unsqueeze(1)  # Ensure proper dimensions if needed\n",
    "            else:\n",
    "                boxes = None\n",
    "\n",
    "\n",
    "\n",
    "            '''test'''\n",
    "            with torch.no_grad():\n",
    "\n",
    "                \"\"\" image encoder \"\"\"\n",
    "                backbone_out = net.forward_image(imgs)\n",
    "                _, vision_feats, vision_pos_embeds, _ = net._prepare_backbone_features(backbone_out)\n",
    "                B = vision_feats[-1].size(1) \n",
    "\n",
    "                \"\"\" memory condition \"\"\"\n",
    "                if len(memory_bank_list) == 0:\n",
    "                    vision_feats[-1] = vision_feats[-1] + torch.nn.Parameter(torch.zeros(1, B, net.hidden_dim)).to(device=\"cuda\")\n",
    "                    vision_pos_embeds[-1] = vision_pos_embeds[-1] + torch.nn.Parameter(torch.zeros(1, B, net.hidden_dim)).to(device=\"cuda\")\n",
    "\n",
    "                else:\n",
    "                    for element in memory_bank_list:\n",
    "                        maskmem_features = element[0]\n",
    "                        maskmem_pos_enc = element[1]\n",
    "                        to_cat_memory.append(maskmem_features.cuda(non_blocking=True).flatten(2).permute(2, 0, 1))\n",
    "                        to_cat_memory_pos.append(maskmem_pos_enc.cuda(non_blocking=True).flatten(2).permute(2, 0, 1))\n",
    "                        to_cat_image_embed.append((element[3]).cuda(non_blocking=True)) # image_embed\n",
    "                        \n",
    "                    memory_stack_ori = torch.stack(to_cat_memory, dim=0)\n",
    "                    memory_pos_stack_ori = torch.stack(to_cat_memory_pos, dim=0)\n",
    "                    image_embed_stack_ori = torch.stack(to_cat_image_embed, dim=0)\n",
    "\n",
    "                    vision_feats_temp = vision_feats[-1].permute(1, 0, 2).reshape(B, -1, 64, 64) \n",
    "                    vision_feats_temp = vision_feats_temp.reshape(B, -1)\n",
    "\n",
    "                    image_embed_stack_ori = F.normalize(image_embed_stack_ori, p=2, dim=1)\n",
    "                    vision_feats_temp = F.normalize(vision_feats_temp, p=2, dim=1)\n",
    "                    similarity_scores = torch.mm(image_embed_stack_ori, vision_feats_temp.t()).t()\n",
    "\n",
    "                    similarity_scores = F.softmax(similarity_scores, dim=1) \n",
    "                    sampled_indices = torch.multinomial(similarity_scores, num_samples=B, replacement=True).squeeze(1)  # Shape [batch_size, 16]\n",
    "\n",
    "                    memory_stack_ori_new = (memory_stack_ori[sampled_indices].squeeze(3).permute(1, 2, 0, 3))\n",
    "                    memory = memory_stack_ori_new.reshape(-1, memory_stack_ori_new.size(2), memory_stack_ori_new.size(3))\n",
    "\n",
    "                    memory_pos_stack_new = (memory_pos_stack_ori[sampled_indices].squeeze(3).permute(1, 2, 0, 3))\n",
    "                    memory_pos = memory_pos_stack_new.reshape(-1, memory_stack_ori_new.size(2), memory_stack_ori_new.size(3))\n",
    "\n",
    "\n",
    "\n",
    "                    vision_feats[-1] = net.memory_attention(\n",
    "                        curr=[vision_feats[-1]],\n",
    "                        curr_pos=[vision_pos_embeds[-1]],\n",
    "                        memory=memory,\n",
    "                        memory_pos=memory_pos,\n",
    "                        num_obj_ptr_tokens=0\n",
    "                        )\n",
    "\n",
    "                feats = [feat.permute(1, 2, 0).reshape(B, -1, *feat_size) \n",
    "                        for feat, feat_size in zip(vision_feats[::-1], feat_sizes[::-1])][::-1]\n",
    "                \n",
    "                image_embed = feats[-1]\n",
    "                high_res_feats = feats[:-1]\n",
    "\n",
    "                \"\"\" prompt encoder \"\"\"\n",
    "                if (ind%5) == 0:\n",
    "                    flag = True\n",
    "                    # points = (coords_torch, labels_torch)\n",
    "\n",
    "                else:\n",
    "                    flag = False\n",
    "                    points = None\n",
    "\n",
    "                se, de = net.sam_prompt_encoder(\n",
    "                    points=None, \n",
    "                    boxes=boxes,\n",
    "                    masks=None,\n",
    "                    batch_size=B,\n",
    "                )\n",
    "\n",
    "                low_res_multimasks, iou_predictions, sam_output_tokens, object_score_logits = net.sam_mask_decoder(\n",
    "                    image_embeddings=image_embed,\n",
    "                    image_pe=net.sam_prompt_encoder.get_dense_pe(), \n",
    "                    sparse_prompt_embeddings=se,\n",
    "                    dense_prompt_embeddings=de, \n",
    "                    multimask_output=False, \n",
    "                    repeat_image=False,  \n",
    "                    high_res_features = high_res_feats\n",
    "                )\n",
    "\n",
    "                # prediction\n",
    "                pred = F.interpolate(low_res_multimasks,size=(args['out_size'],args['out_size']))\n",
    "                high_res_multimasks = F.interpolate(low_res_multimasks, size=(args['image_size'], args['image_size']),\n",
    "                                                mode=\"bilinear\", align_corners=False)\n",
    "            \n",
    "                \"\"\" memory encoder \"\"\"\n",
    "                maskmem_features, maskmem_pos_enc = net._encode_new_memory( \n",
    "                    current_vision_feats=vision_feats,\n",
    "                    feat_sizes=feat_sizes,\n",
    "                    pred_masks_high_res=high_res_multimasks[:, 0:1, :, :],\n",
    "                    is_mask_from_pts=flag)  \n",
    "                    \n",
    "                maskmem_features = maskmem_features.to(torch.bfloat16)\n",
    "                maskmem_features = maskmem_features.to(device=GPUdevice, non_blocking=True)\n",
    "                maskmem_pos_enc = maskmem_pos_enc[0].to(torch.bfloat16)\n",
    "                maskmem_pos_enc = maskmem_pos_enc.to(device=GPUdevice, non_blocking=True)\n",
    "\n",
    "\n",
    "                \"\"\" memory bank \"\"\"\n",
    "                if len(memory_bank_list) < 16:\n",
    "                    for batch in range(maskmem_features.size(0)):\n",
    "                        memory_bank_list.append([(maskmem_features[batch].unsqueeze(0)),\n",
    "                                                 (maskmem_pos_enc[batch].unsqueeze(0)),\n",
    "                                                 iou_predictions[batch, 0],\n",
    "                                                 image_embed[batch].reshape(-1).detach()])\n",
    "                \n",
    "                else:\n",
    "                    for batch in range(maskmem_features.size(0)):\n",
    "                        \n",
    "                        memory_bank_maskmem_features_flatten = [element[0].reshape(-1) for element in memory_bank_list]\n",
    "                        memory_bank_maskmem_features_flatten = torch.stack(memory_bank_maskmem_features_flatten)\n",
    "\n",
    "                        memory_bank_maskmem_features_norm = F.normalize(memory_bank_maskmem_features_flatten, p=2, dim=1)\n",
    "                        current_similarity_matrix = torch.mm(memory_bank_maskmem_features_norm,\n",
    "                                                             memory_bank_maskmem_features_norm.t())\n",
    "\n",
    "                        current_similarity_matrix_no_diag = current_similarity_matrix.clone()\n",
    "                        diag_indices = torch.arange(current_similarity_matrix_no_diag.size(0))\n",
    "                        current_similarity_matrix_no_diag[diag_indices, diag_indices] = float('-inf')\n",
    "\n",
    "                        single_key_norm = F.normalize(maskmem_features[batch].reshape(-1), p=2, dim=0).unsqueeze(1)\n",
    "                        similarity_scores = torch.mm(memory_bank_maskmem_features_norm, single_key_norm).squeeze()\n",
    "                        min_similarity_index = torch.argmin(similarity_scores) \n",
    "                        max_similarity_index = torch.argmax(current_similarity_matrix_no_diag[min_similarity_index])\n",
    "\n",
    "                        if similarity_scores[min_similarity_index] < current_similarity_matrix_no_diag[min_similarity_index][max_similarity_index]:\n",
    "                            if iou_predictions[batch, 0] > memory_bank_list[max_similarity_index][2] - 0.1:\n",
    "                                memory_bank_list.pop(max_similarity_index) \n",
    "                                memory_bank_list.append([(maskmem_features[batch].unsqueeze(0)),\n",
    "                                                         (maskmem_pos_enc[batch].unsqueeze(0)),\n",
    "                                                         iou_predictions[batch, 0],\n",
    "                                                         image_embed[batch].reshape(-1).detach()])\n",
    "\n",
    "                # binary mask and calculate loss, iou, dice\n",
    "                # total_loss += lossfunc(pred, masks)\n",
    "                # pred = (pred> 0.5).float()\n",
    "                # # For disc (first mask)\n",
    "                # dice_disc = eval_dice(pred[:, 0, :, :], masks[:, 0, :, :])\n",
    "                # iou_disc = eval_iou(pred[:, 0, :, :], masks[:, 0, :, :])\n",
    "                # # For cup (second mask)\n",
    "                # dice_cup = eval_dice(pred[:, 1, :, :], masks[:, 1, :, :])\n",
    "                # iou_cup = eval_iou(pred[:, 1, :, :], masks[:, 1, :, :])\n",
    "\n",
    "                loss_disc = lossfunc(pred[:, 0, :, :], masks[:, 0, :, :])\n",
    "                loss_cup  = lossfunc(pred[:, 1, :, :], masks[:, 1, :, :])\n",
    "                dice_disc = dice_score(torch.sigmoid(pred[:, 0, :, :]), masks[:, 0, :, :])\n",
    "                dice_cup  = dice_score(torch.sigmoid(pred[:, 1, :, :]), masks[:, 1, :, :])\n",
    "\n",
    "                loss = .3 * loss_disc + .7 * loss_cup\n",
    "                dice = (dice_disc + dice_cup) / 2\n",
    "\n",
    "                # weight the loss from the cup more because it is worse in predictions\n",
    "                loss = .3 * loss_disc + .7 * loss_cup\n",
    "                pbar.set_postfix(**{'loss (batch)': loss.item()})\n",
    "                total_loss += loss.item()\n",
    "                total_dice += dice.item()\n",
    "\n",
    "\n",
    "                '''vis images'''\n",
    "                # if ind % args.vis == 0:\n",
    "                #     namecat = 'Test'\n",
    "                #     for na in name:\n",
    "                #         img_name = na\n",
    "                #         namecat = namecat + img_name + '+'\n",
    "                #     vis_image(imgs,pred, masks, os.path.join(args.path_helper['sample_path'], namecat+'epoch+' +str(epoch) + '.jpg'), reverse=False, points=None)\n",
    "            # Cleanup large intermediate tensors that are no longer needed.\n",
    "            del backbone_out, vision_feats, vision_pos_embeds, feats, image_embed, high_res_feats, se, de, low_res_multimasks, iou_predictions, sam_output_tokens, object_score_logits, pred\n",
    "\n",
    "            # Optionally free cached memory.\n",
    "            if ind % 10 == 0:  # For instance, every 10 iterations\n",
    "                torch.cuda.empty_cache()    \n",
    "            pbar.update()\n",
    "\n",
    "    return total_loss/ n_val, total_dice / n_val#, tuple([total_eiou/n_val, total_dice/n_val])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_image_path(path: str) -> str:\n",
    "    \"\"\"\n",
    "    Pulls the file name from a file path.\n",
    "\n",
    "    Args:\n",
    "        path (str): file path\n",
    "\n",
    "    Returns:\n",
    "        str: the file name\n",
    "    \"\"\"\n",
    "    split_path = path.split(\"/\")\n",
    "    return split_path[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the fundus images and ground truth masks\n",
    "origa_path = os.path.join('..', '..', \"data\", \"ORIGA\")\n",
    "images_path = os.path.join(origa_path, \"Images_Square\")\n",
    "masks_path = os.path.join(origa_path, \"Masks_Square\")\n",
    "\n",
    "img_filenames = sorted(os.listdir(images_path))\n",
    "mask_filenames = sorted(os.listdir(masks_path))\n",
    "\n",
    "# Read in the bounding boxes\n",
    "bb_df = pd.read_csv(\"../../data/ORIGA/bounding_boxes.csv\")\n",
    "bb_df['image_path'] = bb_df['image_path'].apply(update_image_path)\n",
    "\n",
    "# Update the bounding box coordinates based on the image size. Boxes were created on 512x512 images\n",
    "if med_sam_2.image_size == 256:\n",
    "    bb_df[['x1', 'y1', 'x2', 'y2']] //= 2\n",
    "elif med_sam_2.image_size == 1024:\n",
    "    bb_df[['x1', 'y1', 'x2', 'y2']] *= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train, validation, and test sets (70, 15, 15)\n",
    "train_imgs, temp_imgs, train_masks, temp_masks = train_test_split(\n",
    "    img_filenames, mask_filenames, test_size=0.3, random_state=42)\n",
    "\n",
    "val_imgs, test_imgs, val_masks, test_masks = train_test_split(\n",
    "    temp_imgs, temp_masks, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "n_workers = 4\n",
    "\n",
    "# Load raw data into custom PyTorch datasets\n",
    "train_set = GlaucomaDatasetBoundingBoxes(images_path, masks_path, train_imgs, train_masks, bb_df, 1024)\n",
    "val_set = GlaucomaDatasetBoundingBoxes(images_path, masks_path, val_imgs, val_masks, bb_df, 1024)\n",
    "test_set = GlaucomaDatasetBoundingBoxes(images_path, masks_path, test_imgs, test_masks, bb_df, 1024)\n",
    "\n",
    "# Load datasets into PyTorch DataLoaders\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, num_workers=n_workers, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, num_workers=n_workers, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, num_workers=n_workers, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "args = {\n",
    "    'out_size': 1024,\n",
    "    'image_size': 1024,\n",
    "    'memory_bank_size': 16,\n",
    "    'lr': 1e-4\n",
    "}\n",
    "\n",
    "# initialize Adam optimizer\n",
    "optimizer = optim.Adam(med_sam_2.parameters(), lr=args['lr'], betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "\n",
    "for param in med_sam_2.image_encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Set to max 50 epochs\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_sam_2 = med_sam_2.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/114 [00:00<?, ?img/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "se: torch.Size([4, 2, 256])\n",
      "de: torch.Size([4, 256, 64, 64])\n",
      "image_embed: torch.Size([4, 16, 64, 64])\n",
      "image_embeddings shape: torch.Size([4, 16, 64, 64])\n",
      "dense_prompt_embeddings shape: torch.Size([4, 256, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (16) must match the size of tensor b (256) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      8\u001b[39m val_loss_list = [\u001b[32m0\u001b[39m] * epochs\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[32m     11\u001b[39m     \u001b[38;5;66;03m# Run one training epoch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     train_loss, train_dice = \u001b[43mtrain_sam\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmed_sam_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mTrain loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m || @ epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Train Dice: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_dice\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m || @ epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     14\u001b[39m     train_loss_list[epoch] = train_loss\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 152\u001b[39m, in \u001b[36mtrain_sam\u001b[39m\u001b[34m(args, net, optimizer, train_loader, epoch)\u001b[39m\n\u001b[32m    149\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mimage_embed:\u001b[39m\u001b[33m'\u001b[39m, image_embed.shape)\n\u001b[32m    151\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m'''train mask decoder'''\u001b[39;00m       \n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m low_res_multimasks, iou_predictions, sam_output_tokens, object_score_logits = \u001b[43mnet\u001b[49m\u001b[43m.\u001b[49m\u001b[43msam_mask_decoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimage_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_embed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimage_pe\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m.\u001b[49m\u001b[43msam_prompt_encoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_dense_pe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m        \u001b[49m\u001b[43msparse_prompt_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdense_prompt_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mde\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmultimask_output\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# args.multimask_output if you want multiple masks\u001b[39;49;00m\n\u001b[32m    158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepeat_image\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# the image is already batched\u001b[39;49;00m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhigh_res_features\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mhigh_res_feats\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[38;5;66;03m# dimension hint for your future use\u001b[39;00m\n\u001b[32m    162\u001b[39m \u001b[38;5;66;03m# low_res_multimasks: torch.Size([batch, multimask_output, 256, 256])\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[38;5;66;03m# iou_predictions.shape:torch.Size([batch, multimask_output])\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    167\u001b[39m \n\u001b[32m    168\u001b[39m \u001b[38;5;66;03m# resize prediction\u001b[39;00m\n\u001b[32m    169\u001b[39m pred = F.interpolate(low_res_multimasks,size=(args[\u001b[33m'\u001b[39m\u001b[33mout_size\u001b[39m\u001b[33m'\u001b[39m],args[\u001b[33m'\u001b[39m\u001b[33mout_size\u001b[39m\u001b[33m'\u001b[39m]))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/sfs/gpfs/tardis/home/skk8kc/Documents/Capstone/Glaucoma-Screening-Capstone/Medical-SAM2/sam2_train/modeling/sam/mask_decoder.py:136\u001b[39m, in \u001b[36mMaskDecoder.forward\u001b[39m\u001b[34m(self, image_embeddings, image_pe, sparse_prompt_embeddings, dense_prompt_embeddings, multimask_output, repeat_image, high_res_features)\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    111\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    112\u001b[39m     image_embeddings: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    118\u001b[39m     high_res_features: Optional[List[torch.Tensor]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    119\u001b[39m ) -> Tuple[torch.Tensor, torch.Tensor]:\n\u001b[32m    120\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    121\u001b[39m \u001b[33;03m    Predict masks given image and prompt embeddings.\u001b[39;00m\n\u001b[32m    122\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    134\u001b[39m \u001b[33;03m      torch.Tensor: batched SAM token for mask output\u001b[39;00m\n\u001b[32m    135\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     masks, iou_pred, mask_tokens_out, object_score_logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict_masks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimage_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimage_pe\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_pe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m        \u001b[49m\u001b[43msparse_prompt_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43msparse_prompt_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdense_prompt_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdense_prompt_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepeat_image\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepeat_image\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhigh_res_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhigh_res_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m     \u001b[38;5;66;03m# Retain backward compatibility if needed\u001b[39;00m\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_mask_tokens == \u001b[32m2\u001b[39m:\n\u001b[32m    147\u001b[39m         \u001b[38;5;66;03m# Our case: optic disk and cup\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/sfs/gpfs/tardis/home/skk8kc/Documents/Capstone/Glaucoma-Screening-Capstone/Medical-SAM2/sam2_train/modeling/sam/mask_decoder.py:213\u001b[39m, in \u001b[36mMaskDecoder.predict_masks\u001b[39m\u001b[34m(self, image_embeddings, image_pe, sparse_prompt_embeddings, dense_prompt_embeddings, repeat_image, high_res_features)\u001b[39m\n\u001b[32m    211\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mimage_embeddings shape:\u001b[39m\u001b[33m\"\u001b[39m, src.shape)\n\u001b[32m    212\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mdense_prompt_embeddings shape:\u001b[39m\u001b[33m\"\u001b[39m, dense_prompt_embeddings.shape)\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m src = \u001b[43msrc\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mdense_prompt_embeddings\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[32m    215\u001b[39m     image_pe.size(\u001b[32m0\u001b[39m) == \u001b[32m1\u001b[39m\n\u001b[32m    216\u001b[39m ), \u001b[33m\"\u001b[39m\u001b[33mimage_pe should have size 1 in batch dim (from `get_dense_pe()`)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    217\u001b[39m pos_src = torch.repeat_interleave(image_pe, tokens.shape[\u001b[32m0\u001b[39m], dim=\u001b[32m0\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (16) must match the size of tensor b (256) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# Initialize early stopping conditions\n",
    "best_val_dice = float('-inf')\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "\n",
    "# Initialize loss lists to save loss at each epoch\n",
    "train_loss_list = [0] * epochs\n",
    "val_loss_list = [0] * epochs\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Run one training epoch\n",
    "    train_loss, train_dice = train_sam(args, med_sam_2, optimizer, train_loader, epoch)\n",
    "    print(f'Train loss: {train_loss} || @ epoch {epoch}. Train Dice: {train_dice} || @ epoch {epoch}.')\n",
    "    train_loss_list[epoch] = train_loss\n",
    "\n",
    "    # Run one validation loop\n",
    "    val_loss, val_dice = validation_sam(args, val_loader, epoch, med_sam_2)\n",
    "    print(f'Validation loss: {val_loss} || @ epoch {epoch}. Validation dice: {val_dice} || @ epoch {epoch}.')\n",
    "    val_loss_list[epoch] = val_loss\n",
    "\n",
    "    # Check if our model is better than the best trained model\n",
    "    if val_dice > best_val_dice:\n",
    "        best_val_dice = val_dice\n",
    "        state_dict_to_save = med_sam_2.state_dict()\n",
    "        patience_counter = 0  # Reset patience counter\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"No improvement. Patience: {patience_counter}/{patience}\")\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered @ epoch {epoch}.\")\n",
    "            break\n",
    "\n",
    "# Run a final test evaluation on a holdout set\n",
    "med_sam_2.load_state_dict(state_dict_to_save)\n",
    "test_loss, test_dice = validation_sam(args, test_loader, 0, med_sam_2)\n",
    "print(f'Test loss: {test_loss}. Test Dice: {test_dice}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best trained model\n",
    "torch.save(state_dict_to_save, \"./medsam2-two-mask-512x512-50-epochs-weighted-loss-2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the weights from the best trained model\n",
    "med_sam_2.load_state_dict(torch.load(\"medsam2-two-mask-512x512-50-epochs-weighted-loss-2.pth\", weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the predictor object from the trained Medical-SAM2 model\n",
    "predictor = SAM2ImagePredictor(med_sam_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in a single image\n",
    "img_path = \"../../data/ORIGA/Images_Square/465.jpg\"\n",
    "img = Image.open(img_path).resize((512, 512))\n",
    "img = np.array(img.convert('RGB'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the image for prediction. This must be done when using the predictor object to perform inference\n",
    "predictor.set_image(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the single image using the associated prompt\n",
    "masks, scores, logits = predictor.predict(box=pack['bbox'][1], multimask_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the predicted masks\n",
    "show_masks(img, masks, scores, box_coords=pack['bbox'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = './data/Clinic/Images/Subject (14).png'\n",
    "img = Image.open(img_path).resize((512, 512))\n",
    "img = np.array(img.convert('RGB'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.set_image(img)\n",
    "masks, scores, logits = predictor.predict(box=np.array([260, 65, 350, 160]), multimask_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_masks(img, masks, scores, box_coords=np.array([260, 65, 350, 160]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medsam2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
